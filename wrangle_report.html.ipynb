{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle and Analyze Data (project 5)\n",
    "Introduction\n",
    "This project aims to wrangle data from the WeRateDogs Twitter account using Python. This Twitter account runs humorous commentary and rate dogs over a base mark of ten, although typical scores are greater than baseline.\n",
    "The goal of this project is to wrangle the WeRateDogs Twitter account data to gather all neccessary data, assess and clean the dataset to create interesting and trustworthy analyses and visualizations. \n",
    "\n",
    "### Gathering \n",
    "A dataset of WeRateDogs's twitter archive containig basic tweet data (tweet ID, timestamp, text, etc.) uptil 1st, August 2017  was provided by Udacity (as obtained from WeRateDogs), however numerical data information such as retweet count and favourite count were not included and the provided csv file contained retweets which duplicates the data.\n",
    "Also the tweet image predictions file for prediction and classification of images in each tweet as predicted by a neural network created by Udacity was hosted on Udacity's servers. The file was downloaded  programmatically using python Requests library on the following (URL of the file: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv), and then written to a tsv file named image_predictions.\n",
    "Lastly,to gather information not provided due to the basic-ness of twitter archive dataset, Twitter Api was queried and the tweet id provided in the twitter archive datset was used to obtain the twitter json. Tweet_id, time_stamp, rewteet counts and favorites counts were then extracted from the json.\n",
    "\n",
    "### Assessing\n",
    "\n",
    "To obtain correct and accurate insight from the gathered data, the data has to be clean and tidy. the gathered data was assessed for quality and tidiness check. The dataset was assessed visually and programmatically using inbuilt panda method such as info, describe,value_counts, sample and duplicate. Following issues were discovered upon assessment.\n",
    "\n",
    "Quality issues\n",
    "twitter archive data:\n",
    "1. Incorrect datatype for time stamp Change the timestamp, retweeted_status_timestamp \n",
    "2. data entry with multiple dog_stage\n",
    "3. Duplicated entries(retweets)\n",
    "4. Missing values in the following columns  in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp\n",
    "5. Entries that are not dogs\n",
    "6. Source column contains html tags\n",
    "7. Rating denominator contains values less than and greater than 10\n",
    "8. Rating Numerator contains values that is far from median and mean\n",
    "9. Name column contains lowercase values and odd names like , such, a, an, the, very, unacceptable e.t.c\n",
    "\n",
    "image_prediction file:\n",
    "1. duplicate data(jpg_url column)\n",
    "2. Columns with  p1_dog,p2_dog contains (-_)\n",
    "\n",
    "Tidiness:\n",
    "Twitter archive data\n",
    "1. The variable for the dog's stage (dogoo, floofer, pupper, puppo) is spread in different columns\n",
    "\n",
    "#### image_df:\n",
    "1. The variable for Confidence interval,prediction number and dog prediction is spread in different columns\n",
    "2. This data set is part of the same observational unit as the data in the archive_df \n",
    "\n",
    "#### df_tweet_json:\n",
    "1. This data set is also part of the same observational unit as the data in the archive_df \n",
    "\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "The cleaning exercise was done using the define,code and test technique. the following activities were done:\n",
    "1. Change the timestamp, retweeted_status_timestamp to correct datetime format for all dataset\n",
    "2. Create one column for the various dog types: doggo, floofer, pupper, puppo, 'doggo, puppo', 'doggo, pupper', 'doggo, floofer' and delete rows with multiple dog stage \n",
    "3. create  column for correctly predicted breed, corresponding prediction and confidence interval \n",
    "4. Delete retweets\n",
    "5. Remove columns no longer needed: in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp\n",
    "6. Merge the copies of the three dataframes\n",
    "7. Extract names from text where possible and delete when neccessary for rows with odd and lower case names\n",
    "9. Delete rows where items are not dogs\n",
    "9. Delete rows with duplicated jpg.url\n",
    "10. Remove HTML tags from source column\n",
    "11. Standardise dog rating \n",
    "12. Removal of (-_) from dog prediction column\n",
    "13. Convert the tweet_id in master_df into object type \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
